{"cells":[{"cell_type":"markdown","metadata":{"id":"wRqQgHYP5tkz"},"source":["# This notebook is for the evaluation results of CS224N project"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f8TFG9va5hYn","executionInfo":{"status":"ok","timestamp":1647233100709,"user_tz":420,"elapsed":386594,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}},"outputId":"d281b0b2-e2fd-48d5-9e89-5839d0d04283"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FHVnzJiW5rJC","executionInfo":{"status":"ok","timestamp":1647233100711,"user_tz":420,"elapsed":29,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}},"outputId":"75922105-cb9a-48c9-806b-b0a18aabf846"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Clean CS224N folder\n"]}],"source":["%cd /content/drive/MyDrive/Clean CS224N folder"]},{"cell_type":"markdown","metadata":{"id":"dp2iAiJ753_L"},"source":["# Beicheng's fine-tuned T5 abstractive model\n","Fine-tuned on FB data set"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hLHYl2q05sKw","executionInfo":{"status":"ok","timestamp":1647233110857,"user_tz":420,"elapsed":10161,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}},"outputId":"56f2c452-791c-4223-da7e-44a9ff277716"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[?25l\r\u001b[K     |▎                               | 10 kB 33.7 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 23.2 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 7.9 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n","Collecting bert-score\n","  Downloading bert_score-0.3.11-py3-none-any.whl (60 kB)\n","\u001b[K     |████████████████████████████████| 60 kB 4.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from bert-score) (21.3)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from bert-score) (1.10.0+cu111)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from bert-score) (3.2.2)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.7/dist-packages (from bert-score) (4.63.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bert-score) (2.23.0)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from bert-score) (1.3.5)\n","Collecting transformers>=3.0.0numpy\n","  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 16.4 MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->bert-score) (3.0.7)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->bert-score) (1.21.5)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->bert-score) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.1->bert-score) (1.15.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->bert-score) (3.10.0.2)\n","Collecting tokenizers!=0.11.3,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 61.7 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert-score) (3.6.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 71.7 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 6.6 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 51.6 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert-score) (4.11.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert-score) (2019.12.20)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=3.0.0numpy->bert-score) (3.7.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert-score) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert-score) (1.3.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.0numpy->bert-score) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.0numpy->bert-score) (1.1.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, bert-score\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed bert-score-0.3.11 huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.17.0\n"]}],"source":["!pip install sentencepiece\n","!pip install bert-score"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"obb0awfc53dc","executionInfo":{"status":"ok","timestamp":1647233120621,"user_tz":420,"elapsed":9786,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}},"outputId":"6cda1ae2-f4c4-4892-ca98-314afd125449"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 1.7 MB 7.9 MB/s \n","\u001b[K     |████████████████████████████████| 181 kB 88.3 MB/s \n","\u001b[K     |████████████████████████████████| 144 kB 92.0 MB/s \n","\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n","\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install transformers -q\n","!pip install wandb -q"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"P8hQ8pgv6BO6","executionInfo":{"status":"ok","timestamp":1647233127955,"user_tz":420,"elapsed":7357,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"outputs":[],"source":["# Importing stock libraries\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Importing the T5 modules from huggingface/transformers\n","# from transformers import T5Tokenizer, T5ForConditionalGeneration\n","# from transformers import T5ForConditionalGeneration\n","# from transformers import AutoTokenizer as T5Tokenizer\n","# from transformers import AutoModelWithLMHead as T5ForConditionalGeneration\n","from transformers import AutoModelWithLMHead, AutoTokenizer\n","# AutoModelWithLMHead\n","from bert_score import score\n","\n","# tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n","\n","# model = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n","\n","# WandB – Import the wandb library\n","import wandb"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AZU2zVpV6Dbr","executionInfo":{"status":"ok","timestamp":1647233127955,"user_tz":420,"elapsed":56,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}},"outputId":"017d23f8-91e6-4f00-e6ef-907e1422b603"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Mar 14 04:45:26 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["# Checking out the GPU we have access to. This is output is from the google colab version. \n","!nvidia-smi"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"jvw_bAuG6F7z","executionInfo":{"status":"ok","timestamp":1647233127956,"user_tz":420,"elapsed":41,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"outputs":[],"source":["# # Setting up the device for GPU usage\n","from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'"]},{"cell_type":"markdown","metadata":{"id":"uUKqC5uO6Ufk"},"source":["### Preparing the Dataset for data processing: Class\n","\n","We will start with creation of Dataset class - This defines how the text is pre-processed before sending it to the neural network. This dataset will be used the the Dataloader method that will feed  the data in batches to the neural network for suitable training and processing. \n","The Dataloader and Dataset will be used inside the `main()`.\n","Dataset and Dataloader are constructs of the PyTorch library for defining and controlling the data pre-processing and its passage to neural network. For further reading into Dataset and Dataloader read the [docs at PyTorch](https://pytorch.org/docs/stable/data.html)\n","\n","#### *CustomDataset* Dataset Class\n","- This class is defined to accept the Dataframe as input and generate tokenized output that is used by the **T5** model for training. \n","- We are using the **T5** tokenizer to tokenize the data in the `text` and `ctext` column of the dataframe. \n","- The tokenizer uses the ` batch_encode_plus` method to perform tokenization and generate the necessary outputs, namely: `source_id`, `source_mask` from the actual text and `target_id` and `target_mask` from the summary text.\n","- To read further into the tokenizer, [refer to this document](https://huggingface.co/transformers/model_doc/t5.html#t5tokenizer)\n","- The *CustomDataset* class is used to create 2 datasets, for training and for validation.\n","- *Training Dataset* is used to fine tune the model: **80% of the original data**\n","- *Validation Dataset* is used to evaluate the performance of the model. The model has not seen this data during training. \n","\n","#### Dataloader: Called inside the `main()`\n","- Dataloader is used to for creating training and validation dataloader that load data to the neural network in a defined manner. This is needed because all the data from the dataset cannot be loaded to the memory at once, hence the amount of data loaded to the memory and then passed to the neural network needs to be controlled.\n","- This control is achieved using the parameters such as `batch_size` and `max_len`.\n","- Training and Validation dataloaders are used in the training and validation part of the flow respectively"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"V1okJoze6MaB","executionInfo":{"status":"ok","timestamp":1647233127956,"user_tz":420,"elapsed":39,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"outputs":[],"source":["# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n","\n","class CustomDataset(Dataset): ### seq2seq from ctext to text\n","\n","    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.source_len = source_len\n","        self.summ_len = summ_len\n","        self.text = self.data.text\n","        self.ctext = self.data.ctext\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        ctext = str(self.ctext[index])\n","        ctext = ' '.join(ctext.split())\n","\n","        text = str(self.text[index])\n","        text = ' '.join(text.split())\n","\n","        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n","        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n","\n","        source_ids = source['input_ids'].squeeze()\n","        source_mask = source['attention_mask'].squeeze()\n","        target_ids = target['input_ids'].squeeze()\n","        target_mask = target['attention_mask'].squeeze()\n","\n","        return {\n","            'source_ids': source_ids.to(dtype=torch.long), \n","            'source_mask': source_mask.to(dtype=torch.long), \n","            'target_ids': target_ids.to(dtype=torch.long),\n","            'target_ids_y': target_ids.to(dtype=torch.long)\n","        }"]},{"cell_type":"markdown","metadata":{"id":"u0KQkBes6-eq"},"source":["<a id='section03'></a>\n","### Fine Tuning the Model: Function\n","\n","Here we define a training function that trains the model on the training dataset created above, specified number of times (EPOCH), An epoch defines how many times the complete data will be passed through the network. \n","\n","This function is called in the `main()`\n","\n","Following events happen in this function to fine tune the neural network:\n","- The epoch, tokenizer, model, device details, testing_ dataloader and optimizer are passed to the `train ()` when its called from the `main()`\n","- The dataloader passes data to the model based on the batch size.\n","- `language_model_labels` are calculated from the `target_ids` also, `source_id` and `attention_mask` are extracted.\n","- The model outputs first element gives the loss for the forward pass. \n","- Loss value is used to optimize the weights of the neurons in the network.\n","- After every 10 steps the loss value is logged in the wandb service. This log is then used to generate graphs for analysis. Such as [these](https://app.wandb.ai/abhimishra-91/transformers_tutorials_summarization?workspace=user-abhimishra-91)\n","- After every 500 steps the loss value is printed in the console."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"xDxM46Ww6WYF","executionInfo":{"status":"ok","timestamp":1647233127956,"user_tz":420,"elapsed":37,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"outputs":[],"source":["# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n","# The model is put into train mode and then we wnumerate over the training loader and passed to the defined network \n","def train(epoch, tokenizer, model, device, loader, optimizer):\n","    model.train()\n","    for _,data in enumerate(loader, 0):\n","        y = data['target_ids'].to(device, dtype = torch.long)\n","        y_ids = y[:, :-1].contiguous()\n","        # lm_labels = y[:, 1:].clone().detach()\n","        # lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n","        lm_labels = y[:, :].clone().detach()\n","        lm_labels[lm_labels == tokenizer.pad_token_id] = -100\n","        ids = data['source_ids'].to(device, dtype = torch.long)\n","        mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","        # outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n","        # outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n","        outputs = model(input_ids = ids, attention_mask = mask, labels=lm_labels)\n","        loss = outputs[0]\n","        \n","        if _%10 == 0:\n","            wandb.log({\"Training Loss\": loss.item()})\n","\n","        if _%500==0:\n","            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        # xm.optimizer_step(optimizer)\n","        # xm.mark_step()"]},{"cell_type":"markdown","metadata":{"id":"x4pbAOXP7D_n"},"source":["<a id='section04'></a>\n","### Validating the Model Performance: Function\n","\n","During the validation stage we pass the unseen data(Testing Dataset), trained model, tokenizer and device details to the function to perform the validation run. This step generates new summary for dataset that it has not seen during the training session. \n","\n","This function is called in the `main()`\n","\n","This unseen data is the 20% of `news_summary.csv` which was seperated during the Dataset creation stage. \n","During the validation stage the weights of the model are not updated. We use the generate method for generating new text for the summary. \n","\n","It depends on the `Beam-Search coding` method developed for sequence generation for models with LM head. \n","\n","The generated text and originally summary are decoded from tokens to text and returned to the `main()`"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"2o7uXxhW7Bog","executionInfo":{"status":"ok","timestamp":1647233127957,"user_tz":420,"elapsed":36,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"outputs":[],"source":["def validate(epoch, tokenizer, model, device, loader):\n","    model.eval()\n","    predictions = []\n","    actuals = []\n","    with torch.no_grad():\n","        for _, data in enumerate(loader, 0):\n","            y = data['target_ids'].to(device, dtype = torch.long)\n","            ids = data['source_ids'].to(device, dtype = torch.long)\n","            mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","            generated_ids = model.generate(\n","                input_ids = ids,\n","                attention_mask = mask, \n","                max_length=150, \n","                num_beams=2,\n","                repetition_penalty=2.5, \n","                length_penalty=1.0, \n","                early_stopping=True\n","                )\n","            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n","            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n","            if _%100==0:\n","                print(f'Completed {_}')\n","\n","            predictions.extend(preds)\n","            actuals.extend(target)\n","    return predictions, actuals"]},{"cell_type":"markdown","metadata":{"id":"r83RclNC7Ooo"},"source":["### Load our data set"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"IVy_Rk_m7GUn","executionInfo":{"status":"ok","timestamp":1647233128477,"user_tz":420,"elapsed":553,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}},"outputId":"38c8825a-dbbd-45ef-bd15-94a78d9864cd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                summary  \\\n","0     Gadot is Israeli, and pronounces her last name...   \n","1                                               Ghost.    \n","2     \"...I always thought I was the most outsider, ...   \n","3     She's allowing people to visit her guesthouse,...   \n","4     He was traded to Nashville to be closer to his...   \n","...                                                 ...   \n","1307           It's Rachel. Meghan is her middle name.    \n","1308                           The McDonald's Big Mac.    \n","1309  French, due to the large growth of French-spea...   \n","1310  He got a 5 on his AP Art portfolio and got sel...   \n","1311                      As long as any other iPhone.    \n","\n","                                                   link  \\\n","0     http://www.slate.com/blogs/browbeat/2017/05/30...   \n","1     http://www.mirror.co.uk/tv/tv-news/games-thron...   \n","2     http://www.cinemablend.com/news/1653270/why-di...   \n","3                              http://ctrylv.co/66pB7xu   \n","4     http://faithtap.com/7882/hockey-heartthrob-mik...   \n","...                                                 ...   \n","1307  https://www.yahoo.com/lifestyle/meghan-markle-...   \n","1308  https://www.businessinsider.com/mcdonalds-wend...   \n","1309  https://www.usatoday.com/story/news/world/2014...   \n","1310  https://www.gaystarnews.com/article/teacher-sn...   \n","1311  https://www.theverge.com/circuitbreaker/2017/9...   \n","\n","                                                article  \\\n","0     With the release of Wonder Woman, star Gal Gad...   \n","1     Season 7 of Game of Thrones is teased in new t...   \n","2     While Diane Keaton might not be the first name...   \n","3     Ree Drummond superfans have no shortage of pla...   \n","4     “I think just time spending with them, being w...   \n","...                                                 ...   \n","1307  “Meghan Markle” is the kind of charming, allit...   \n","1308  Get the Insider App A personalized feed, summa...   \n","1309  Is French the language of the future?\\n\\nCorre...   \n","1310  Jasper Behrends is a trans teenager who lives ...   \n","1311  I usually spend about $1,300 on a new computer...   \n","\n","                                                  title  \n","0     How to pronounce Gal Gadot, the star of Wonder...  \n","1     Game Of Thrones star tragically dies just week...  \n","2     Why Diane Keaton Hadn’t Watched The Godfather ...  \n","3     The Pioneer Woman Is Letting People Tour Her R...  \n","4                                     FaithTap Archives  \n","...                                                 ...  \n","1307  Meghan Markle's real name isn't actually Megha...  \n","1308  We compared McDonald's, Wendy's, and Burger Ki...  \n","1309              Is French the language of the future?  \n","1310  Teacher snubs trans teen’s art project but wha...  \n","1311                How long should a $999 iPhone last?  \n","\n","[1257 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-8f182c61-92f8-4f59-a459-c3faef02ef8c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>summary</th>\n","      <th>link</th>\n","      <th>article</th>\n","      <th>title</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Gadot is Israeli, and pronounces her last name...</td>\n","      <td>http://www.slate.com/blogs/browbeat/2017/05/30...</td>\n","      <td>With the release of Wonder Woman, star Gal Gad...</td>\n","      <td>How to pronounce Gal Gadot, the star of Wonder...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Ghost.</td>\n","      <td>http://www.mirror.co.uk/tv/tv-news/games-thron...</td>\n","      <td>Season 7 of Game of Thrones is teased in new t...</td>\n","      <td>Game Of Thrones star tragically dies just week...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>\"...I always thought I was the most outsider, ...</td>\n","      <td>http://www.cinemablend.com/news/1653270/why-di...</td>\n","      <td>While Diane Keaton might not be the first name...</td>\n","      <td>Why Diane Keaton Hadn’t Watched The Godfather ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>She's allowing people to visit her guesthouse,...</td>\n","      <td>http://ctrylv.co/66pB7xu</td>\n","      <td>Ree Drummond superfans have no shortage of pla...</td>\n","      <td>The Pioneer Woman Is Letting People Tour Her R...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>He was traded to Nashville to be closer to his...</td>\n","      <td>http://faithtap.com/7882/hockey-heartthrob-mik...</td>\n","      <td>“I think just time spending with them, being w...</td>\n","      <td>FaithTap Archives</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1307</th>\n","      <td>It's Rachel. Meghan is her middle name.</td>\n","      <td>https://www.yahoo.com/lifestyle/meghan-markle-...</td>\n","      <td>“Meghan Markle” is the kind of charming, allit...</td>\n","      <td>Meghan Markle's real name isn't actually Megha...</td>\n","    </tr>\n","    <tr>\n","      <th>1308</th>\n","      <td>The McDonald's Big Mac.</td>\n","      <td>https://www.businessinsider.com/mcdonalds-wend...</td>\n","      <td>Get the Insider App A personalized feed, summa...</td>\n","      <td>We compared McDonald's, Wendy's, and Burger Ki...</td>\n","    </tr>\n","    <tr>\n","      <th>1309</th>\n","      <td>French, due to the large growth of French-spea...</td>\n","      <td>https://www.usatoday.com/story/news/world/2014...</td>\n","      <td>Is French the language of the future?\\n\\nCorre...</td>\n","      <td>Is French the language of the future?</td>\n","    </tr>\n","    <tr>\n","      <th>1310</th>\n","      <td>He got a 5 on his AP Art portfolio and got sel...</td>\n","      <td>https://www.gaystarnews.com/article/teacher-sn...</td>\n","      <td>Jasper Behrends is a trans teenager who lives ...</td>\n","      <td>Teacher snubs trans teen’s art project but wha...</td>\n","    </tr>\n","    <tr>\n","      <th>1311</th>\n","      <td>As long as any other iPhone.</td>\n","      <td>https://www.theverge.com/circuitbreaker/2017/9...</td>\n","      <td>I usually spend about $1,300 on a new computer...</td>\n","      <td>How long should a $999 iPhone last?</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1257 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f182c61-92f8-4f59-a459-c3faef02ef8c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8f182c61-92f8-4f59-a459-c3faef02ef8c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8f182c61-92f8-4f59-a459-c3faef02ef8c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}],"source":["import pandas as pd\n","df = pd.read_csv('SCB_all_v2.csv')\n","#Check what preprocessing (Beicheng uses pruned data set)\n","df = df.dropna() #remove nones\n","df['summary'] = df['summary'].str.replace('#StopClickbait', '')\n","df"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"atgmM8cY7sPl","executionInfo":{"status":"ok","timestamp":1647233128478,"user_tz":420,"elapsed":17,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"outputs":[],"source":["df['ctext'] = \"context: \"+df.article +\" <question for context: \"+ df.title +\" </s>\"\n","df['text'] = df.summary"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JuPz7t0T72oS","executionInfo":{"status":"ok","timestamp":1647233128479,"user_tz":420,"elapsed":17,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}},"outputId":"7357add7-a9bc-4c85-f47f-5ddfc5fa4fb4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1257"]},"metadata":{},"execution_count":13}],"source":["len(df)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"v6tF_WYN8BIy","executionInfo":{"status":"ok","timestamp":1647233128480,"user_tz":420,"elapsed":15,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"outputs":[],"source":["#df.to_excel(\"temp.xlsx\")"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0,"referenced_widgets":["5c9a5fda535f4b4194ef4a7898a21e5f","d92d86501ff442b599b44cd8730ec806","9cfa5f36e512443d8fefc33be18a4a18","09b1328f97c947caad44b52f0b83790e","dd1285a946f24ad6bfe44a7f03d76f13","93f546a85d76462088febe058d2f341f","6553ddb40d4447b9addde16c00703138","5a6b769598f044c9863a5cbb250e7f0d","f74fc7b7c20c4893aef19cbb9b806d27","d8f4e94a3dd9406a8ca7c4fb46efcc8d","be8530d3dd6d43e585a19a8f5a586767","8b7acadb9587492bb7df4d26ddadb144","0c0d8968278c45f3b85076e9b10f00bc","745e3c9e7f8c483d955257de1866c4ac","36eaf216718e40bda2ebb57cf10521a9","6342f6b825924bcd85e976ac693fcba1","d6d78b0a9197406c929480447da802d7","e561c8d5be224a4894475b3a386becb3","4076312209044da08912d81d0bce5846","3478645ceea54148852b6da93c0a2c54","0b30ff1828e4426fb1543d3a442f2fda","653f24da68304fff8cb022d2f6f6661f","df806574eed148c29b3d39a2d75e939a","4f00a842eefd4e9ab1fd0e99b641554c","e0cbcc28cbbc4f0d99273d2f37b08a76","f462bf8e6f7544e7a9261bb369905ad2","56a7d80569ff48cda99b64bdfca00038","e820c59105d94b209ff1ac836f630ee8","4ca2fc11dab541afa8702fa08d8c95ce","b32ae5b613dc44579509fe7b7f6e6bbf","e1f636a175e4494ea049716eb3855640","3d97e150f9f740db86833b64b26148f4","9e50b0a5eb504c1f85c172abbf2b1f2c","fe34effe95b840da90c844e07c661a5d","84ba50993de44f5b87838108835d3904","76c48a39407441278661b7591829ba2c","cd23bf50c1064afea5b945c08e15fde6","9c2d8ac3f95340e4b8af15b7cf29db5a","5c2a75aec5c247c6a01ea9c0775c6eb8","0535f886711f493f9c378ae7f227198e","e4871347143f440db0d80a92244c47d4","d09168e73a32444986c6d5eedd31dede","32932efb08d14ea0a5ca89e364d031c8","941272021d264aeabb473aaf60cbd316"]},"id":"qop-k_M077YO","executionInfo":{"status":"ok","timestamp":1647233143525,"user_tz":420,"elapsed":15059,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}},"outputId":"8ee23972-eccd-4354-deba-12a779922ab6"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.12.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/Clean CS224N folder/wandb/run-20220314_044533-9g4zlaru</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/janetzh/transformers_tutorials_summarization/runs/9g4zlaru\" target=\"_blank\">neapolitan-pie-19</a></strong> to <a href=\"https://wandb.ai/janetzh/transformers_tutorials_summarization\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c9a5fda535f4b4194ef4a7898a21e5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b7acadb9587492bb7df4d26ddadb144"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df806574eed148c29b3d39a2d75e939a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe34effe95b840da90c844e07c661a5d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n","The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"]},{"output_type":"stream","name":"stdout","text":["FULL Dataset: (1257, 6)\n","TRAIN Dataset: (1006, 6)\n","TEST Dataset: (126, 6)\n"]}],"source":["# WandB – Initialize a new run\n","wandb.init(project=\"transformers_tutorials_summarization\")\n","\n","# WandB – Config is a variable that holds and saves hyperparameters and inputs\n","# Defining some key variables that will be used later on in the training  \n","config = wandb.config          # Initialize config\n","config.TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n","config.VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n","config.TRAIN_EPOCHS = 20        # number of epochs to train (default: 10)\n","config.VAL_EPOCHS = 1 \n","config.LEARNING_RATE = 1e-5    # learning rate (default: 0.01)\n","config.SEED = 42               # random seed (default: 42)\n","config.MAX_LEN = 1024 \n","config.SUMMARY_LEN = 150 \n","\n","# Set random seeds and deterministic pytorch for reproducibility\n","torch.manual_seed(config.SEED) # pytorch random seed\n","np.random.seed(config.SEED) # numpy random seed\n","torch.backends.cudnn.deterministic = True\n","\n","# tokenzier for encoding the text\n","# tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n","tokenizer = AutoTokenizer.from_pretrained(\"tuner007/t5_abs_qa\")\n","\n","\n","\n","# Importing and Pre-Processing the domain data\n","# Selecting the needed columns only. \n","# Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \n","# df = pd.read_csv('./data/news_summary.csv',encoding='latin-1')\n","# df = df[['text','ctext']]\n","# df.ctext = 'summarize: ' + df.ctext\n","# print(df.head())\n","\n","\n","# Creation of Dataset and Dataloader\n","# Defining the train size. So 90% of the rest of data (10% already taken for test, so 0.81) will be used for training and the rest will be used for validation. \n","train_size = 0.8\n","val_size = 0.1 ### the rest goes to test\n","train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n","# val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n","val_test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n","train_dataset = train_dataset.reset_index(drop=True)\n","val_dataset=val_test_dataset.sample(frac=val_size / (1-train_size),random_state = config.SEED)\n","test_dataset=val_test_dataset.drop(val_dataset.index).reset_index(drop=True)\n","val_dataset = val_dataset.reset_index(drop=True)\n","\n","print(\"FULL Dataset: {}\".format(df.shape))\n","print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","print(\"TEST Dataset: {}\".format(val_dataset.shape))\n","\n","\n","# Creating the Training and Validation dataset for further creation of Dataloader\n","training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","test_set = CustomDataset(test_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","\n","# Defining the parameters for creation of dataloaders\n","train_params = {\n","    'batch_size': config.TRAIN_BATCH_SIZE,\n","    'shuffle': True,\n","    'num_workers': 0\n","    }\n","\n","val_params = {\n","    'batch_size': config.VALID_BATCH_SIZE,\n","    'shuffle': False,\n","    'num_workers': 0\n","    } ### also used for test set\n","\n","# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n","training_loader = DataLoader(training_set, **train_params)\n","val_loader = DataLoader(val_set, **val_params)\n","test_loader = DataLoader(test_set, **val_params)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lVeYZTSD7_r4","executionInfo":{"status":"ok","timestamp":1647233143527,"user_tz":420,"elapsed":19,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}},"outputId":"4144726e-bd17-464c-ba9c-e52fc467b3b3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1006, 126, 125)"]},"metadata":{},"execution_count":16}],"source":["len(train_dataset), len(val_dataset), len(test_dataset)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"O5iiDw-t0uqa","executionInfo":{"status":"ok","timestamp":1647233144347,"user_tz":420,"elapsed":831,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"outputs":[],"source":["train_dataset.to_csv('./train_dataset.csv')\n","val_dataset.to_csv('./val_dataset.csv')\n","test_dataset.to_csv('./test_dataset.csv')"]},{"cell_type":"markdown","source":["### Load model parameters\n","go straight to this using presaved model params"],"metadata":{"id":"FqNHKqWq4AZr"}},{"cell_type":"code","source":["#model = Model()\n","model = AutoModelWithLMHead.from_pretrained(\"tuner007/t5_abs_qa\")\n","model = model.to(device)\n","path = 'model_20epochs_state_dict'\n","model.load_state_dict(torch.load(path), strict=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":138,"referenced_widgets":["29440dec6cb741de9256e90f3365f40c","07d9009374144059a1368eab12e97e8c","2711fd2f0eeb467e944a56ec47292a06","2d313f1ad96b4d708653728af6067134","1650bd16656e4536879898eab5fcbff2","3c4ffe234d2f426aa6a89a791a2deb56","50ed8924774e49029832e53f853e9001","250a4adee0bc4291af531170e85003c9","f708c452c65d4a018d75fdfe202ae52a","bebac842b932475bbdcdd55c69c0bc56","cf0027f3d7314e4790f01b7ebf95e12a"]},"id":"tI7xIbA3BFa7","executionInfo":{"status":"ok","timestamp":1647233212035,"user_tz":420,"elapsed":67696,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}},"outputId":"c5e8f79b-a5d6-4bc7-f81f-d2fba68b715c"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:882: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n","  FutureWarning,\n","The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29440dec6cb741de9256e90f3365f40c"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# # Setting up the device for GPU usage\n","from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'"],"metadata":{"id":"I0Y1-EpZDMPW","executionInfo":{"status":"ok","timestamp":1647233212036,"user_tz":420,"elapsed":24,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["i=1"],"metadata":{"id":"0Q-bWsaU_VNJ","executionInfo":{"status":"ok","timestamp":1647233212038,"user_tz":420,"elapsed":22,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["model.to('cpu')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VJ7TyNm9DrJ7","executionInfo":{"status":"ok","timestamp":1647233212462,"user_tz":420,"elapsed":444,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}},"outputId":"6d8d781a-f0c1-4d54-c364-dd8115510e3d"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 768)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",")"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["def get_answer(question, context): ### check\n","  input_text = \"context: %s <question for context: %s </s>\" % (context,question)\n","  features = tokenizer([input_text], return_tensors='pt')\n","  out = model.generate(input_ids=features['input_ids'].to(device), attention_mask=features['attention_mask'].to(device))\n","  # print(out[0])\n","  # for i in out[0]:\n","  #   print(i,tokenizer.decode(i))\n","  return tokenizer.decode(out[0].detach()) ### test if we dont store the output"],"metadata":{"id":"PBwNFlmfxYHw","executionInfo":{"status":"ok","timestamp":1647233568020,"user_tz":420,"elapsed":1,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["train_dataset = pd.read_csv('train_dataset.csv')\n","val_dataset = pd.read_csv('val_dataset.csv')\n","test_dataset = pd.read_csv('test_dataset.csv')\n","\n","print(len(val_dataset))\n","print(len(test_dataset))\n","\n","whole_dataset = pd.concat([train_dataset,test_dataset,val_dataset],ignore_index=True)\n","whole_dataset = whole_dataset[['summary','link','article','title']]\n","whole_dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":458},"id":"g9GwrE24xekx","executionInfo":{"status":"ok","timestamp":1647233593546,"user_tz":420,"elapsed":365,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}},"outputId":"3bfbc6ef-2ae3-46eb-f1ca-1d35f6468f98"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["126\n","125\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                summary  \\\n","0                        September 16th. #stopclickbait   \n","1                  June Moon, aka Enchantress...maybe.    \n","2                                        Nathan Keyes.    \n","3       His dad and grandpa--Michael and Kirk Douglas.    \n","4                                         In the pool.    \n","...                                                 ...   \n","1252                     The CDC doesn't recommend it.    \n","1253  So the crew can assess the surroundings in an ...   \n","1254  Industry standards are eliminating/reducing si...   \n","1255  Australians aren't actually going anywhere...t...   \n","1256  It's an invasive species called giant hogweed....   \n","\n","                                                   link  \\\n","0     http://www.unilad.co.uk/viral/this-is-the-most...   \n","1     http://www.cinemablend.com/new/Who-Main-Villai...   \n","2     http://www.instyle.com/news/britney-spears-bio...   \n","3     https://www.queerty.com/youll-never-guess-mich...   \n","4     https://www.clickorlando.com/news/2018/08/02/v...   \n","...                                                 ...   \n","1252  https://canoe.com/news/weird/will-wearing-wate...   \n","1253  https://theculturetrip.com/north-america/usa/a...   \n","1254  https://www.usatoday.com/story/news/nation/201...   \n","1255  http://www.iflscience.com/environment/australi...   \n","1256           http://realfarmacy.com/dont-touch-plant/   \n","\n","                                                article  \\\n","0     NBC\\n\\nIf like myself you’re forever forgettin...   \n","1     The next DCEU movie to hit theaters will mark ...   \n","2     Naked dresses are perhaps the most impressive ...   \n","3     You’ll Never Guess What (Or Who) Michael Dougl...   \n","4     MAPLE VALLEY, WASH. – A Washington dad pulled ...   \n","...                                                 ...   \n","1252  As if wearing face masks, washing your hands a...   \n","1253  Book your bucket list adventure here with TRIP...   \n","1254  You won't be able to buy some corded blinds st...   \n","1255  Hold on tight, Australians – on New Year’s Day...   \n","1256  They should teach this in school but unfortuna...   \n","\n","                                                  title  \n","0         This Is The Most Common Birthday In The World  \n","1          Who The Main Villain Of Suicide Squad May Be  \n","2     Lifetime's \"Britney\" Casts Role of Justin Timb...  \n","3     You’ll Never Guess What (Or Who) Michael Dougl...  \n","4     Viral video: You'll never guess where this guy...  \n","...                                                 ...  \n","1252  Will wearing water jugs on your head combat co...  \n","1253  Here's Why Flight Attendants Ask You to Raise ...  \n","1254  You won't be able to buy some corded blinds st...  \n","1255  Australia Will Suddenly Move 1.8 Meters North ...  \n","1256  Warning: If You See This Plant- Whatever You D...  \n","\n","[1257 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-9ba8c30e-c190-4ed9-9aae-73ba36e4516e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>summary</th>\n","      <th>link</th>\n","      <th>article</th>\n","      <th>title</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>September 16th. #stopclickbait</td>\n","      <td>http://www.unilad.co.uk/viral/this-is-the-most...</td>\n","      <td>NBC\\n\\nIf like myself you’re forever forgettin...</td>\n","      <td>This Is The Most Common Birthday In The World</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>June Moon, aka Enchantress...maybe.</td>\n","      <td>http://www.cinemablend.com/new/Who-Main-Villai...</td>\n","      <td>The next DCEU movie to hit theaters will mark ...</td>\n","      <td>Who The Main Villain Of Suicide Squad May Be</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Nathan Keyes.</td>\n","      <td>http://www.instyle.com/news/britney-spears-bio...</td>\n","      <td>Naked dresses are perhaps the most impressive ...</td>\n","      <td>Lifetime's \"Britney\" Casts Role of Justin Timb...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>His dad and grandpa--Michael and Kirk Douglas.</td>\n","      <td>https://www.queerty.com/youll-never-guess-mich...</td>\n","      <td>You’ll Never Guess What (Or Who) Michael Dougl...</td>\n","      <td>You’ll Never Guess What (Or Who) Michael Dougl...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>In the pool.</td>\n","      <td>https://www.clickorlando.com/news/2018/08/02/v...</td>\n","      <td>MAPLE VALLEY, WASH. – A Washington dad pulled ...</td>\n","      <td>Viral video: You'll never guess where this guy...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1252</th>\n","      <td>The CDC doesn't recommend it.</td>\n","      <td>https://canoe.com/news/weird/will-wearing-wate...</td>\n","      <td>As if wearing face masks, washing your hands a...</td>\n","      <td>Will wearing water jugs on your head combat co...</td>\n","    </tr>\n","    <tr>\n","      <th>1253</th>\n","      <td>So the crew can assess the surroundings in an ...</td>\n","      <td>https://theculturetrip.com/north-america/usa/a...</td>\n","      <td>Book your bucket list adventure here with TRIP...</td>\n","      <td>Here's Why Flight Attendants Ask You to Raise ...</td>\n","    </tr>\n","    <tr>\n","      <th>1254</th>\n","      <td>Industry standards are eliminating/reducing si...</td>\n","      <td>https://www.usatoday.com/story/news/nation/201...</td>\n","      <td>You won't be able to buy some corded blinds st...</td>\n","      <td>You won't be able to buy some corded blinds st...</td>\n","    </tr>\n","    <tr>\n","      <th>1255</th>\n","      <td>Australians aren't actually going anywhere...t...</td>\n","      <td>http://www.iflscience.com/environment/australi...</td>\n","      <td>Hold on tight, Australians – on New Year’s Day...</td>\n","      <td>Australia Will Suddenly Move 1.8 Meters North ...</td>\n","    </tr>\n","    <tr>\n","      <th>1256</th>\n","      <td>It's an invasive species called giant hogweed....</td>\n","      <td>http://realfarmacy.com/dont-touch-plant/</td>\n","      <td>They should teach this in school but unfortuna...</td>\n","      <td>Warning: If You See This Plant- Whatever You D...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1257 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ba8c30e-c190-4ed9-9aae-73ba36e4516e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9ba8c30e-c190-4ed9-9aae-73ba36e4516e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9ba8c30e-c190-4ed9-9aae-73ba36e4516e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["print(whole_dataset['title'][i])\n","print(whole_dataset['article'][i])\n","print(get_answer(whole_dataset['title'][i],whole_dataset['article'][i])[6:-4])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":834},"id":"sD5GIa3MxaP4","executionInfo":{"status":"error","timestamp":1647233596491,"user_tz":420,"elapsed":361,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}},"outputId":"23069c9f-9ef4-48d0-c14c-00d0d98202e5"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (1026 > 512). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["Who The Main Villain Of Suicide Squad May Be\n","The next DCEU movie to hit theaters will mark one of the weirdest and boldest comic book adaptations ever committed to the silver screen. Suicide Squad hasn’t even premiered yet, and it’s already obvious that the film has taken every possible left turn to set itself apart from what we expect from this genre. David Ayer has a vision in mind, and from what we’ve seen, it seems delightfully fresh.\n","\n","However, despite the fact that we’re beyond excited for the upcoming release of Suicide Squad, we seriously know nothing about the film’s actual villain. Sure, it will be undeniably awesome to see these characters coalesce on the silver screen for the first time, but the trailers and marketing materials have done very little to inform us of the overarching conflict. What looming threat arises that causes Amanda Waller to bring this group of psychos and killers together? We’ve gone through the details, and come up with five potential big bads that could take center stage once Suicide Squad hits theaters on August 5.\n","\n","Enchantress\n","\n","Based upon everything we’ve seen from the Suicide Squad marketing campaign thus far, it seems as though June Moone a.k.a Enchantress (Cara Delevingne) has the strongest chance of taking on the film’s villain role. She’s featured prominently in the film’s trailers – consistently looking quite sinister – and her magic-based powers seem to line up with our understanding that the Suicide Squad will fight magical forces not quite within the realm of science. However, in some many ways, June Moone and Enchantress are two completely separate entities, so it remains to be seen whether or not the titular Squad will have to take down both of them, or just the evil witch persona.\n","\n","Joker\n","\n","Recent reports from early screenings of Suicide Squad seem to point to the fact that Jared Leto’s Joker may have no more than 20 minutes of screen time in the final cut of the film. That being said, that doesn’t rule out the possibility that he’s the mastermind behind all of the destruction and carnage we see. It wouldn’t feel too surprising to learn that he’s pulling the strings and manipulating everyone from behind-the-scenes. It could be to lure out the Batman, win back Harley Quinn, or simply to cause chaos everywhere he goes. At this point, nothing from this character would surprise us.\n","\n","Tattooed Man\n","\n","Although the Tattooed Man’s presence in Suicide Squad has not been officially confirmed, many eagle-eyed comic book fans seem to have picked up on the notion that rapper Common will indeed portray the relatively obscure character. Typically known as a Green Lantern villain, the Tattooed Man has the ability to manifest constructs from tattoos inked into his skin. The official designation of Common’s character is \"Monster T,\" and a monster featured briefly in the film’s trailers seems to match up with this particular villain’s known abilities, so all eyes are on the Tattooed Man to potentially be the one to cause a major headache for the titular team of super villains.\n","\n","Lex Luthor\n","\n","Numerous unconfirmed rumors have persisted for quite some time that Jesse Eisenberg’s Lex Luthor would potentially pop up at some point during the events of Suicide Squad for a cameo. He’s currently rotting in a jail cell, and for Amanda Waller to have him transferred to Belle Reve would make a great deal of sense. For him to be the ultimate mastermind behind the events of the film would most certainly help the universe establish his conniving intelligence in a way that Batman V Superman: Dawn of Justice simply failed to do. It would also help expand the reach of the DCEU, making an instant connection between Dawn of Justice and David Ayer's Suicide Squad.\n","\n","Someone Else Entirely\n","\n","Like I already said: Suicide Squad seems like one of the best-kept secrets in the entire superhero genre right now. All of the aforementioned characters seem poised to take on villainous roles in some form or another, but there could very well be another threat looming in the wings. Perhaps new facts of the DC lore will receive ample exploration with this one supervillain team-up. The film will reportedly take place in Midway City, so maybe the Thanagarians will show up and cause some trouble. At this point, we will just have to wait and see how it all pans out when Suicide Squad hits theaters on August 5.\n","\n","This poll is no longer available.\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-1ac6726f1df7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhole_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhole_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhole_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwhole_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-23-32626cf4927d>\u001b[0m in \u001b[0;36mget_answer\u001b[0;34m(question, context)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"context: %s <question for context: %s </s>\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;31m# print(out[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m# for i in out[0]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0;31m# and added to `model_kwargs`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0;32m-> 1103\u001b[0;31m                 \u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m             )\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_input_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_outputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"You have to initialize the model with valid token embeddings\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)"]}]},{"cell_type":"code","source":["for data, target in load_data.train_loader:\n","    data = data.cuda()\n","    target = target.cuda()"],"metadata":{"id":"HYkldmBUxzSw","executionInfo":{"status":"error","timestamp":1647233678128,"user_tz":420,"elapsed":404,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}},"outputId":"3ad3b785-9d3f-4abb-f726-d70c3b25d37d","colab":{"base_uri":"https://localhost:8080/","height":200}},"execution_count":27,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-6c5f3e777f07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'load_data' is not defined"]}]},{"cell_type":"code","source":["    text = val_dataset['article'][i]\n","    question = val_dataset['title'][i]\n","    encoding = tokenizer.encode_plus(question, text, return_tensors=\"pt\")\n","    input_ids = encoding[\"input_ids\"]\n","\n","    # default is local attention everywhere\n","    # the forward method will automatically set global attention on question tokens\n","    attention_mask = encoding[\"attention_mask\"]\n","\n","    start_scores, end_scores = model(input_ids, attention_mask=attention_mask)\n","    all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n","    answer_tokens = all_tokens[torch.argmax(start_scores) :torch.argmax(end_scores)+1]\n","    answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n","    val_dataset[\"Finetuned e20 Text\"][i] = answer"],"metadata":{"id":"7u289Cpd2ZQJ","colab":{"base_uri":"https://localhost:8080/","height":397},"executionInfo":{"status":"error","timestamp":1647233212967,"user_tz":420,"elapsed":511,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}},"outputId":"2a566583-aee0-4a97-ff1d-d6cba8da2b80"},"execution_count":22,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-777965bc216e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mstart_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mall_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0manswer_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_scores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1645\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         )\n\u001b[1;32m   1649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0merr_msg_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"decoder_\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: You have to specify either decoder_input_ids or decoder_inputs_embeds"]}]},{"cell_type":"code","source":["val_dataset[\"Finetuned e20 Text\"] = \"\"\n","\n","for i in range(len(val_dataset)):\n","  try:\n","    print(i)\n","    text = val_dataset['article'][i]\n","    question = val_dataset['title'][i]\n","    encoding = tokenizer.encode_plus(question, text, return_tensors=\"pt\")\n","    input_ids = encoding[\"input_ids\"]\n","\n","    # default is local attention everywhere\n","    # the forward method will automatically set global attention on question tokens\n","    attention_mask = encoding[\"attention_mask\"]\n","\n","    start_scores, end_scores = model(input_ids, attention_mask=attention_mask)\n","    all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n","    answer_tokens = all_tokens[torch.argmax(start_scores) :torch.argmax(end_scores)+1]\n","    answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n","    val_dataset[\"Finetuned e20 Text\"][i] = answer\n","  except:\n","    pass"],"metadata":{"id":"6cbasbus3_Ai","executionInfo":{"status":"aborted","timestamp":1647233212961,"user_tz":420,"elapsed":25,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_dataset"],"metadata":{"id":"K_qD4yXC86Zu","executionInfo":{"status":"aborted","timestamp":1647233212962,"user_tz":420,"elapsed":25,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oBws5RJ495VO"},"source":["# Calculate BERT scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5f0p2nseSljj","executionInfo":{"status":"aborted","timestamp":1647233212962,"user_tz":420,"elapsed":24,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"outputs":[],"source":["epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GXoW4PwbSod7","executionInfo":{"status":"aborted","timestamp":1647233212962,"user_tz":420,"elapsed":24,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"outputs":[],"source":["tmp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rqp6Zjsp919F","executionInfo":{"status":"aborted","timestamp":1647233212963,"user_tz":420,"elapsed":24,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"outputs":[],"source":["tmp = df_val\n","Pb, Rb, Fb = score([str(i) for i in tmp['Actual Text'].tolist()], [str(i) for i in tmp['Generated Text'].tolist()], lang='en')\n","fbscores = Fb.tolist()\n","epochs = [0]*len(Fb)\n","avgs_val = [np.average(fbscores)]\n","for epoch in range(1,21):\n","  Pb, Rb, Fb = score([str(i) for i in tmp['Actual Text'].tolist()], [str(i) for i in tmp[f'Generated Text {epoch}'].tolist()], lang='en')\n","  fbscores += Fb.tolist()\n","  epochs += [epoch]*len(Fb)\n","  avgs_val.append(np.average(Fb.tolist()))\n","sns.lineplot(x=epochs, y=fbscores, ci='sd')\n","plt.xlabel('Epochs')\n","plt.ylabel('Fbert score')\n","plt.savefig('validation_score.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRp0uAjk93il","executionInfo":{"status":"aborted","timestamp":1647233212964,"user_tz":420,"elapsed":25,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"outputs":[],"source":["tmp = df_test\n","Pb, Rb, Fb = score([str(i) for i in tmp['Actual Text'].tolist()], [str(i) for i in tmp['Generated Text'].tolist()], lang='en')\n","fbscores = Fb.tolist()\n","epochs = [0]*len(Fb)\n","avgs_val = [np.average(fbscores)]\n","for epoch in range(1,21):\n","  Pb, Rb, Fb = score([str(i) for i in tmp['Actual Text'].tolist()], [str(i) for i in tmp[f'Generated Text {epoch}'].tolist()], lang='en')\n","  fbscores += Fb.tolist()\n","  epochs += [epoch]*len(Fb)\n","  avgs_val.append(np.average(Fb.tolist()))\n","sns.lineplot(x=epochs, y=fbscores, ci='sd')\n","plt.xlabel('Epochs')\n","plt.ylabel('Fbert score')\n","plt.savefig('validation_score.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1GPQ8Mym-Je1","executionInfo":{"status":"aborted","timestamp":1647233212964,"user_tz":420,"elapsed":24,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"outputs":[],"source":["#Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n","#Saving the dataframe as predictions.csv\n","print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n","for epoch in range(config.VAL_EPOCHS):\n","    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n","    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n","    final_df.to_csv('./predictions.csv')\n","    print('Output Files generated for review')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1mfcv64l-NTp","executionInfo":{"status":"aborted","timestamp":1647233212965,"user_tz":420,"elapsed":25,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"outputs":[],"source":["final_df"]},{"cell_type":"markdown","metadata":{"id":"Ki-XO4KXGrTQ"},"source":["# Print BERTscore"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"glJUQPLpGrAj","executionInfo":{"status":"aborted","timestamp":1647233212965,"user_tz":420,"elapsed":25,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"outputs":[],"source":["tmp = df_test\n","Pb, Rb, Fb = score([str(i) for i in tmp['Actual Text'].tolist()], [str(i) for i in tmp['Generated Text'].tolist()], lang='en')\n","epochs = [0]*len(Fb)\n","print(\"Precision: \"+str(Pb))\n","print(\"Recall: \"+str(Rb))\n","print(\"Fbert: \"+str(Fb))\n","print(\"Mean Precision: \"+str(torch.mean(Pb)))\n","print(\"Mean Recall: \"+str(torch.mean(Rb[~torch.isnan(Rb)])))\n","print(\"Mean Fbert: \"+str(torch.mean(Fb)))\n","\n","for epoch in range(1,21):\n","  Pb, Rb, Fb = score([str(i) for i in tmp['Actual Text'].tolist()], [str(i) for i in tmp[f'Generated Text {epoch}'].tolist()], lang='en')\n","  print(\"Epoch\" + str(epoch))\n","  print(\"Precision: \"+str(Pb))\n","  print(\"Recall: \"+str(Rb))\n","  print(\"Fbert: \"+str(Fb))\n","  print(\"Mean Precision: \"+str(torch.mean(Pb)))\n","  print(\"Mean Recall: \"+str(torch.mean(Rb[~torch.isnan(Rb)])))\n","  print(\"Mean Fbert: \"+str(torch.mean(Fb)))"]},{"cell_type":"markdown","metadata":{"id":"N292qrX2ADdt"},"source":["# Calculate Rogue scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HTuQwM2vVN27","executionInfo":{"status":"aborted","timestamp":1647233212965,"user_tz":420,"elapsed":24,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"outputs":[],"source":["!pip install rouge\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8qM_yedh-fUn","executionInfo":{"status":"aborted","timestamp":1647233212966,"user_tz":420,"elapsed":25,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"outputs":[],"source":["#https://www.programcreek.com/python/example/125541/rouge.Rouge\n","\n","from rouge import Rouge \n","\n","\n","def compute_rouge(predictions, targets):\n","    predictions = [\" \".join(prediction).lower() for prediction in predictions]\n","    predictions = [prediction if prediction else \"EMPTY\" for prediction in predictions]\n","    targets = [\" \".join(target).lower() for target in targets]\n","    targets = [target if target else \"EMPTY\" for target in targets]\n","    rouge = Rouge()\n","    scores = rouge.get_scores(hyps=predictions, refs=targets, avg=True)\n","    return  scores\n","\n","for epoch in range(1,21):\n","    targets = [str(i) for i in tmp['Actual Text'].tolist()]\n","    prediction = [str(i) for i in tmp[f'Generated Text {epoch}'].tolist()]\n","    print(compute_rouge(predictions, targets))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KSgZ2Wj_DDch","executionInfo":{"status":"aborted","timestamp":1647233212966,"user_tz":420,"elapsed":24,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"outputs":[],"source":["!pip install rouge/requirements.txt\n","!pip install rouge-score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xxVnR3b2g163","executionInfo":{"status":"aborted","timestamp":1647233212966,"user_tz":420,"elapsed":24,"user":{"displayName":"Janet Z","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01959738933780546248"}}},"outputs":[],"source":["fmeasure = score['rouge'+ind]\n","        results['precision'].append(precision)\n","        results['recall'].append(recall)\n","        results['fmeasure'].append(fmeasure)\n","    print(\"results['precision']\"+ str(np.mean(results['precision'])))\n","    print(\"results['recall']\"+ str(np.mean(results['recall'])))\n","    print(\"results['fmeasure']\"+ str(np.mean(results['fmeasure'])))"]},{"cell_type":"markdown","metadata":{"id":"M_8hlfPZqctf"},"source":["We could try it on the reddit data set"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","name":"evaluation_results_finetuneT5.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyM1OfiUOd7ghuOE1Ehn4yUI"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5c9a5fda535f4b4194ef4a7898a21e5f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d92d86501ff442b599b44cd8730ec806","IPY_MODEL_9cfa5f36e512443d8fefc33be18a4a18","IPY_MODEL_09b1328f97c947caad44b52f0b83790e"],"layout":"IPY_MODEL_dd1285a946f24ad6bfe44a7f03d76f13"}},"d92d86501ff442b599b44cd8730ec806":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_93f546a85d76462088febe058d2f341f","placeholder":"​","style":"IPY_MODEL_6553ddb40d4447b9addde16c00703138","value":"Downloading: 100%"}},"9cfa5f36e512443d8fefc33be18a4a18":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a6b769598f044c9863a5cbb250e7f0d","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f74fc7b7c20c4893aef19cbb9b806d27","value":25}},"09b1328f97c947caad44b52f0b83790e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8f4e94a3dd9406a8ca7c4fb46efcc8d","placeholder":"​","style":"IPY_MODEL_be8530d3dd6d43e585a19a8f5a586767","value":" 25.0/25.0 [00:00&lt;00:00, 560B/s]"}},"dd1285a946f24ad6bfe44a7f03d76f13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93f546a85d76462088febe058d2f341f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6553ddb40d4447b9addde16c00703138":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a6b769598f044c9863a5cbb250e7f0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f74fc7b7c20c4893aef19cbb9b806d27":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d8f4e94a3dd9406a8ca7c4fb46efcc8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be8530d3dd6d43e585a19a8f5a586767":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8b7acadb9587492bb7df4d26ddadb144":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0c0d8968278c45f3b85076e9b10f00bc","IPY_MODEL_745e3c9e7f8c483d955257de1866c4ac","IPY_MODEL_36eaf216718e40bda2ebb57cf10521a9"],"layout":"IPY_MODEL_6342f6b825924bcd85e976ac693fcba1"}},"0c0d8968278c45f3b85076e9b10f00bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6d78b0a9197406c929480447da802d7","placeholder":"​","style":"IPY_MODEL_e561c8d5be224a4894475b3a386becb3","value":"Downloading: 100%"}},"745e3c9e7f8c483d955257de1866c4ac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4076312209044da08912d81d0bce5846","max":1230,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3478645ceea54148852b6da93c0a2c54","value":1230}},"36eaf216718e40bda2ebb57cf10521a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b30ff1828e4426fb1543d3a442f2fda","placeholder":"​","style":"IPY_MODEL_653f24da68304fff8cb022d2f6f6661f","value":" 1.20k/1.20k [00:00&lt;00:00, 18.9kB/s]"}},"6342f6b825924bcd85e976ac693fcba1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6d78b0a9197406c929480447da802d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e561c8d5be224a4894475b3a386becb3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4076312209044da08912d81d0bce5846":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3478645ceea54148852b6da93c0a2c54":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0b30ff1828e4426fb1543d3a442f2fda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"653f24da68304fff8cb022d2f6f6661f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df806574eed148c29b3d39a2d75e939a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4f00a842eefd4e9ab1fd0e99b641554c","IPY_MODEL_e0cbcc28cbbc4f0d99273d2f37b08a76","IPY_MODEL_f462bf8e6f7544e7a9261bb369905ad2"],"layout":"IPY_MODEL_56a7d80569ff48cda99b64bdfca00038"}},"4f00a842eefd4e9ab1fd0e99b641554c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e820c59105d94b209ff1ac836f630ee8","placeholder":"​","style":"IPY_MODEL_4ca2fc11dab541afa8702fa08d8c95ce","value":"Downloading: 100%"}},"e0cbcc28cbbc4f0d99273d2f37b08a76":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b32ae5b613dc44579509fe7b7f6e6bbf","max":791656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e1f636a175e4494ea049716eb3855640","value":791656}},"f462bf8e6f7544e7a9261bb369905ad2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d97e150f9f740db86833b64b26148f4","placeholder":"​","style":"IPY_MODEL_9e50b0a5eb504c1f85c172abbf2b1f2c","value":" 773k/773k [00:00&lt;00:00, 1.51MB/s]"}},"56a7d80569ff48cda99b64bdfca00038":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e820c59105d94b209ff1ac836f630ee8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ca2fc11dab541afa8702fa08d8c95ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b32ae5b613dc44579509fe7b7f6e6bbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1f636a175e4494ea049716eb3855640":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3d97e150f9f740db86833b64b26148f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e50b0a5eb504c1f85c172abbf2b1f2c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe34effe95b840da90c844e07c661a5d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_84ba50993de44f5b87838108835d3904","IPY_MODEL_76c48a39407441278661b7591829ba2c","IPY_MODEL_cd23bf50c1064afea5b945c08e15fde6"],"layout":"IPY_MODEL_9c2d8ac3f95340e4b8af15b7cf29db5a"}},"84ba50993de44f5b87838108835d3904":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c2a75aec5c247c6a01ea9c0775c6eb8","placeholder":"​","style":"IPY_MODEL_0535f886711f493f9c378ae7f227198e","value":"Downloading: 100%"}},"76c48a39407441278661b7591829ba2c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4871347143f440db0d80a92244c47d4","max":65,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d09168e73a32444986c6d5eedd31dede","value":65}},"cd23bf50c1064afea5b945c08e15fde6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32932efb08d14ea0a5ca89e364d031c8","placeholder":"​","style":"IPY_MODEL_941272021d264aeabb473aaf60cbd316","value":" 65.0/65.0 [00:00&lt;00:00, 1.23kB/s]"}},"9c2d8ac3f95340e4b8af15b7cf29db5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c2a75aec5c247c6a01ea9c0775c6eb8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0535f886711f493f9c378ae7f227198e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e4871347143f440db0d80a92244c47d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d09168e73a32444986c6d5eedd31dede":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"32932efb08d14ea0a5ca89e364d031c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"941272021d264aeabb473aaf60cbd316":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"29440dec6cb741de9256e90f3365f40c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07d9009374144059a1368eab12e97e8c","IPY_MODEL_2711fd2f0eeb467e944a56ec47292a06","IPY_MODEL_2d313f1ad96b4d708653728af6067134"],"layout":"IPY_MODEL_1650bd16656e4536879898eab5fcbff2"}},"07d9009374144059a1368eab12e97e8c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c4ffe234d2f426aa6a89a791a2deb56","placeholder":"​","style":"IPY_MODEL_50ed8924774e49029832e53f853e9001","value":"Downloading: 100%"}},"2711fd2f0eeb467e944a56ec47292a06":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_250a4adee0bc4291af531170e85003c9","max":1187797362,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f708c452c65d4a018d75fdfe202ae52a","value":1187797362}},"2d313f1ad96b4d708653728af6067134":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bebac842b932475bbdcdd55c69c0bc56","placeholder":"​","style":"IPY_MODEL_cf0027f3d7314e4790f01b7ebf95e12a","value":" 1.11G/1.11G [00:49&lt;00:00, 20.6MB/s]"}},"1650bd16656e4536879898eab5fcbff2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c4ffe234d2f426aa6a89a791a2deb56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50ed8924774e49029832e53f853e9001":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"250a4adee0bc4291af531170e85003c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f708c452c65d4a018d75fdfe202ae52a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bebac842b932475bbdcdd55c69c0bc56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf0027f3d7314e4790f01b7ebf95e12a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}